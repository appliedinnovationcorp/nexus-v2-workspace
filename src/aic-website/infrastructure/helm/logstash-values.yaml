---
# Logstash Helm values for AIC Website
# Based on the Elastic Helm chart

# Logstash version
imageTag: "8.8.1"

# Replica count
replicas: 2

# Resource limits and requests
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "1"
    memory: "2Gi"

# Logstash configuration
logstashConfig:
  logstash.yml: |
    http.host: 0.0.0.0
    xpack.monitoring.enabled: true
    xpack.monitoring.elasticsearch.hosts: ["https://aic-elasticsearch-master:9200"]
    xpack.monitoring.elasticsearch.username: logstash_system
    xpack.monitoring.elasticsearch.password: ${ELASTICSEARCH_PASSWORD}
    xpack.monitoring.elasticsearch.ssl.certificate_authority: /usr/share/logstash/config/certs/ca.crt
    queue.type: persisted
    dead_letter_queue.enable: true
    log.level: info

# Logstash pipeline configuration
logstashPipeline:
  main.conf: |
    input {
      # HTTP input for RESTful API logs
      http {
        port => 8080
        codec => "json"
        tags => ["http_input"]
      }
      
      # TCP input for syslog
      tcp {
        port => 5000
        codec => "json"
        tags => ["tcp_input"]
      }
      
      # Beats input for Filebeat
      beats {
        port => 5044
        ssl => false
        tags => ["beats_input"]
      }
      
      # Kafka input for distributed logs
      kafka {
        bootstrap_servers => "kafka-headless.messaging:9092"
        topics => ["logs"]
        codec => "json"
        tags => ["kafka_input"]
        consumer_threads => 4
        group_id => "logstash"
      }
    }
    
    filter {
      # Add kubernetes metadata
      if [kubernetes] {
        mutate {
          add_field => {
            "[@metadata][index]" => "k8s-%{[kubernetes][namespace]}-%{+YYYY.MM.dd}"
          }
        }
      }
      
      # Add timestamp
      date {
        match => [ "timestamp", "ISO8601" ]
        target => "@timestamp"
      }
      
      # Parse JSON if needed
      if [message] =~ /^\{.*\}$/ {
        json {
          source => "message"
        }
      }
      
      # Add service name if available
      if [service] {
        mutate {
          add_field => {
            "[@metadata][service]" => "%{[service][name]}"
          }
        }
      }
      
      # Grok patterns for common log formats
      if [tags] == "syslog" {
        grok {
          match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
        }
      }
      
      # Drop health check logs
      if [request] and [request][url] and [request][url][path] == "/health" {
        drop {}
      }
    }
    
    output {
      # Send all logs to Elasticsearch
      elasticsearch {
        hosts => ["https://aic-elasticsearch-master:9200"]
        user => "logstash_writer"
        password => "${ELASTICSEARCH_PASSWORD}"
        ssl => true
        ssl_certificate_verification => true
        cacert => "/usr/share/logstash/config/certs/ca.crt"
        
        # Dynamic index name based on metadata
        index => "%{[@metadata][index]}-%{+YYYY.MM.dd}"
        
        # Retry configuration
        retry_on_conflict => 5
        action => "index"
      }
      
      # Send specific logs to Kafka for further processing
      if [tags] == "security" {
        kafka {
          bootstrap_servers => "kafka-headless.messaging:9092"
          topic_id => "security-logs"
          codec => json
        }
      }
    }

# Extra environment variables
extraEnvs:
  - name: ELASTICSEARCH_PASSWORD
    valueFrom:
      secretKeyRef:
        name: logstash-credentials
        key: password
  - name: LS_JAVA_OPTS
    value: "-Xmx1g -Xms1g"

# Security settings
securityContext:
  runAsUser: 1000
  fsGroup: 1000

# Service configuration
service:
  type: ClusterIP
  ports:
    - name: beats
      port: 5044
      protocol: TCP
      targetPort: 5044
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: tcp
      port: 5000
      protocol: TCP
      targetPort: 5000
    - name: monitoring
      port: 9600
      protocol: TCP
      targetPort: 9600
  annotations:
    kuma.io/mesh: default

# Persistence configuration
persistence:
  enabled: true
  accessMode: ReadWriteOnce
  size: "20Gi"
  storageClass: "longhorn"

# Pod annotations
podAnnotations:
  kuma.io/mesh: default
  prometheus.io/scrape: "true"
  prometheus.io/port: "9600"
  prometheus.io/path: "/metrics"

# Elasticsearch SSL certificates
extraVolumes:
  - name: elasticsearch-certs
    secret:
      secretName: elasticsearch-security-certs
  - name: logstash-patterns
    configMap:
      name: logstash-patterns
extraVolumeMounts:
  - name: elasticsearch-certs
    mountPath: /usr/share/logstash/config/certs
    readOnly: true
  - name: logstash-patterns
    mountPath: /usr/share/logstash/patterns
    readOnly: true

# Pod affinity for high availability
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app
                operator: In
                values:
                  - logstash
          topologyKey: kubernetes.io/hostname

# Prometheus metrics
prometheus:
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      namespace: "observability"
      labels:
        release: prometheus
      interval: 10s
      scrapeTimeout: 10s

# Health check
livenessProbe:
  httpGet:
    path: /
    port: monitoring
  initialDelaySeconds: 60
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1
readinessProbe:
  httpGet:
    path: /
    port: monitoring
  initialDelaySeconds: 60
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

# Lifecycle hooks
lifecycle:
  postStart:
    exec:
      command:
        - bash
        - -c
        - |
          #!/bin/bash
          # Add any post-start initialization here
